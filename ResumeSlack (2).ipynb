{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88be4b83-4abe-45ee-9079-3aa23b451d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx\n",
    "import re\n",
    "\n",
    "# Check for embedded images\n",
    "def check_images(doc):\n",
    "    image_count = sum(1 for rel in doc.part.rels.values() if \"image\" in rel.reltype)\n",
    "    return image_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c231bfc-0b11-4001-9842-05683991cee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract basic information\n",
    "def extract_basic_info(text):\n",
    "    name_pattern = r\"^(\\w+\\s\\w+)\"\n",
    "    email_pattern = r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\"\n",
    "    phone_pattern = r\"\\b\\d{10}\\b\"\n",
    "    address_pattern = r\"\\d+\\s\\w+\\s\\w+\"\n",
    "\n",
    "    name = re.findall(name_pattern, text)\n",
    "    email = re.findall(email_pattern, text)\n",
    "    phone = re.findall(phone_pattern, text)\n",
    "    address = re.findall(address_pattern, text)\n",
    "\n",
    "    return {\n",
    "        'Name': name[0] if name else 'Not Found',\n",
    "        'Email': email[0] if email else 'Not Found',\n",
    "        'Phone': phone[0] if phone else 'Not Found',\n",
    "        'Address': address[0] if address else 'Not Found'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52d85b26-935f-4fce-9b43-f11900586ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the Experience section\n",
    "def extract_experience(doc):\n",
    "    experience_text = []\n",
    "    recording = False\n",
    "    for para in doc.paragraphs:\n",
    "        if \"experience\" in para.text.lower():\n",
    "            recording = True\n",
    "        if recording:\n",
    "            experience_text.append(para.text)\n",
    "            if para.text.strip() == \"\":\n",
    "                break\n",
    "    return \"\\n\".join(experience_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2373e080-7240-4e8b-873e-f82f639b380a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of pages (Approximation based on character count)\n",
    "def count_pages(doc):\n",
    "    total_chars = sum(len(para.text) for para in doc.paragraphs)\n",
    "    return max(1, total_chars // 1800)  # Rough estimate of 1800 characters per page\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b84d8bdf-35e4-42af-8957-3f81d2b5bc39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the full path to your .docx file:  C:\\Users\\Forhad\\OneDrive\\Documents\\John Doe_Resume.docx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 9 paragraphs from the document!\n",
      "\n",
      "Images Count: 0\n",
      "\n",
      "Basic Information: {'Name': 'John Doe', 'Email': 'john.doe@example.com', 'Phone': '1234567890', 'Address': '123 Main St'}\n",
      "\n",
      "Experience Section:\n",
      "Experience:\n",
      "- Data Scientist at ABC Corp (3 years)\n",
      "Page Count: 1\n"
     ]
    }
   ],
   "source": [
    "# Main pipeline to process the resume\n",
    "def process_resume():\n",
    "    try:\n",
    "        # Load the .docx file\n",
    "        file_path = input(\"Enter the full path to your .docx file: \")\n",
    "        doc = docx.Document(file_path)\n",
    "        print(f\"Loaded {len(doc.paragraphs)} paragraphs from the document!\")\n",
    "\n",
    "        # Combine all paragraphs into a single text\n",
    "        text = '\\n'.join([para.text for para in doc.paragraphs])\n",
    "\n",
    "        # Get embedded images count\n",
    "        image_count = check_images(doc)\n",
    "        print(f\"\\nImages Count: {image_count}\")\n",
    "\n",
    "        # Extract basic information\n",
    "        basic_info = extract_basic_info(text)\n",
    "        print(f\"\\nBasic Information: {basic_info}\")\n",
    "\n",
    "        # Extract experience section\n",
    "        experience_section = extract_experience(doc)\n",
    "        print(f\"\\nExperience Section:\\n{experience_section}\")\n",
    "\n",
    "        # Count the number of pages\n",
    "        page_count = count_pages(doc)\n",
    "        print(f\"Page Count: {page_count}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "# Execute the process_resume function\n",
    "if __name__ == \"__main__\":\n",
    "    process_resume()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d20eb14b-1af9-4f13-8554-f037f03664a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the full path to your .docx file:  C:\\Users\\Forhad\\OneDrive\\Documents\\John Doe_Resume.docx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 9 paragraphs from the document!\n",
      "\n",
      "Images Count: 0\n",
      "\n",
      "Basic Information: {'Name': 'John Doe', 'Email': 'john.doe@example.com', 'Phone': '1234567890', 'Address': '123 Main St'}\n",
      "\n",
      "Experience Section:\n",
      "Experience:\n",
      "- Data Scientist at ABC Corp (3 years)\n",
      "Page Count: 1\n",
      "\n",
      "Full Address Found: Yes\n",
      "\n",
      "Portfolio Links: {'GitHub': 'github.com/johndoe', 'LinkedIn': 'linkedin.com/in/johndoe'}\n"
     ]
    }
   ],
   "source": [
    "import docx\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Check for embedded images\n",
    "def check_images(doc):\n",
    "    image_count = sum(1 for rel in doc.part.rels.values() if \"image\" in rel.reltype)\n",
    "    return image_count\n",
    "\n",
    "# Extract basic information\n",
    "def extract_basic_info(text):\n",
    "    name_pattern = r\"^(\\w+\\s\\w+)\"\n",
    "    email_pattern = r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\"\n",
    "    phone_pattern = r\"\\b\\d{10}\\b\"\n",
    "    address_pattern = r\"\\d+\\s\\w+\\s\\w+\"\n",
    "\n",
    "    name = re.findall(name_pattern, text)\n",
    "    email = re.findall(email_pattern, text)\n",
    "    phone = re.findall(phone_pattern, text)\n",
    "    address = re.findall(address_pattern, text)\n",
    "\n",
    "    return {\n",
    "        'Name': name[0] if name else 'Not Found',\n",
    "        'Email': email[0] if email else 'Not Found',\n",
    "        'Phone': phone[0] if phone else 'Not Found',\n",
    "        'Address': address[0] if address else 'Not Found'\n",
    "    }\n",
    "\n",
    "# Extract the Experience section\n",
    "def extract_experience(doc):\n",
    "    experience_text = []\n",
    "    recording = False\n",
    "    for para in doc.paragraphs:\n",
    "        if \"experience\" in para.text.lower():\n",
    "            recording = True\n",
    "        if recording:\n",
    "            experience_text.append(para.text)\n",
    "            if para.text.strip() == \"\":\n",
    "                break\n",
    "    return \"\\n\".join(experience_text)\n",
    "\n",
    "# Count number of pages (Approximation based on character count)\n",
    "def count_pages(doc):\n",
    "    total_chars = sum(len(para.text) for para in doc.paragraphs)\n",
    "    return max(1, total_chars // 1800)  # Rough estimate of 1800 characters per page\n",
    "\n",
    "# Check for full address\n",
    "def train_address_model():\n",
    "    # Training data\n",
    "    data = [\n",
    "        (\"123 Main St, Springfield\", 1),\n",
    "        (\"456 Elm St, Somecity, CA 98765\", 1),\n",
    "        (\"Main Street\", 0),\n",
    "        (\"Apartment 23, 789 North Ave\", 0)\n",
    "    ]\n",
    "    texts, labels = zip(*data)\n",
    "\n",
    "    # Feature extraction and model training\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X, labels)\n",
    "\n",
    "    return vectorizer, model\n",
    "\n",
    "def check_full_address(text, vectorizer, model):\n",
    "    X = vectorizer.transform([text])\n",
    "    return bool(model.predict(X)[0])\n",
    "\n",
    "# Check for portfolio links\n",
    "def check_portfolio_links(text):\n",
    "    github_pattern = r\"github\\.com/[a-zA-Z0-9_-]+\"\n",
    "    linkedin_pattern = r\"linkedin\\.com/in/[a-zA-Z0-9_-]+\"\n",
    "\n",
    "    github = re.search(github_pattern, text)\n",
    "    linkedin = re.search(linkedin_pattern, text)\n",
    "\n",
    "    return {\n",
    "        'GitHub': github.group(0) if github else 'Not Found',\n",
    "        'LinkedIn': linkedin.group(0) if linkedin else 'Not Found'\n",
    "    }\n",
    "\n",
    "# Main pipeline to process the resume\n",
    "def process_resume():\n",
    "    try:\n",
    "        # Load the .docx file\n",
    "        file_path = input(\"Enter the full path to your .docx file: \")\n",
    "        doc = docx.Document(file_path)\n",
    "        print(f\"Loaded {len(doc.paragraphs)} paragraphs from the document!\")\n",
    "\n",
    "        # Combine all paragraphs into a single text\n",
    "        text = '\\n'.join([para.text for para in doc.paragraphs])\n",
    "\n",
    "        # Get embedded images count\n",
    "        image_count = check_images(doc)\n",
    "        print(f\"\\nImages Count: {image_count}\")\n",
    "\n",
    "        # Extract basic information\n",
    "        basic_info = extract_basic_info(text)\n",
    "        print(f\"\\nBasic Information: {basic_info}\")\n",
    "\n",
    "        # Extract experience section\n",
    "        experience_section = extract_experience(doc)\n",
    "        print(f\"\\nExperience Section:\\n{experience_section}\")\n",
    "\n",
    "        # Count the number of pages\n",
    "        page_count = count_pages(doc)\n",
    "        print(f\"Page Count: {page_count}\")\n",
    "\n",
    "        # Train the address model\n",
    "        vectorizer, address_model = train_address_model()\n",
    "\n",
    "        # Check for full address\n",
    "        has_full_address = check_full_address(text, vectorizer, address_model)\n",
    "        print(f\"\\nFull Address Found: {'Yes' if has_full_address else 'No'}\")\n",
    "\n",
    "        # Check for portfolio links\n",
    "        portfolio_links = check_portfolio_links(text)\n",
    "        print(f\"\\nPortfolio Links: {portfolio_links}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "# Execute the process_resume function\n",
    "if __name__ == \"__main__\":\n",
    "    process_resume()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cec3b9-7451-440f-857c-c8473dfc8508",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
